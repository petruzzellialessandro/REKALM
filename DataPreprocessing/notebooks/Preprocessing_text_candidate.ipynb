{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'lastfm'\n",
    "\n",
    "domain = {\n",
    "    'lastfm': 'music',\n",
    "    'dbbook': 'book',\n",
    "    'movielens': 'movie'\n",
    "}[dataset]\n",
    "import os\n",
    "datapath = os.path.join('..', 'data', dataset)\n",
    "\n",
    "special_token = '<|reserved_special_token_22|>'\n",
    "\n",
    "resp_frac = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import swifter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_train = pd.read_csv(os.path.join(datapath, 'user-item', 'train.tsv'), \\\n",
    "            names=['user', 'item', 'rating'], sep='\\t')\n",
    "\n",
    "user_item_df_test = pd.read_csv(os.path.join(datapath, 'user-item', 'test.tsv'), \\\n",
    "            names=['user', 'item', 'rating'], sep='\\t')\n",
    "\n",
    "\n",
    "df = pd.concat([user_item_df_train, user_item_df_test], ignore_index=True)\n",
    "all_items = df['item'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_random(list_to_split, frac=0.2, margin=0):\n",
    "    random.seed(22)\n",
    "    sample_list = random.sample(list_to_split, int(frac*len(list_to_split))+margin)\n",
    "    complementary_set = set(list_to_split) - set(sample_list)\n",
    "    return list(set(complementary_set)), list(set(sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(22)\n",
    "test_frac = 0.2\n",
    "user_list = df['user'].unique().tolist()\n",
    "user_train, user_test = get_split_random(user_list, frac=test_frac, margin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_train = df[df['user'].isin(user_train)]\n",
    "user_item_df_test = df[df['user'].isin(user_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(user_item_df_train['user'].unique()) - set(user_item_df_test['user'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_test['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_train['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group = user_item_df_train.groupby('user').agg({\n",
    "    'item': list,\n",
    "    'rating': list\n",
    "}).reset_index()\n",
    "test_group = user_item_df_test.groupby('user').agg({\n",
    "    'item': list,\n",
    "    'rating': list\n",
    "}).reset_index()\n",
    "train_group['len'] = train_group['item'].apply(len)\n",
    "test_group['len'] = test_group['item'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_prompt_resp(row, resp_frac):\n",
    "    items = np.array(row['rating'])\n",
    "    index_likes = np.where(items==1)[0].tolist()\n",
    "    index_dilikes = np.where(items==0)[0].tolist()\n",
    "    index_like_prompt = []\n",
    "    index_like_resp = []    \n",
    "    index_dislike_prompt = []\n",
    "    index_dislike_resp = []\n",
    "    resp_frac=resp_frac\n",
    "    if len(index_likes) > 0:\n",
    "        index_like_prompt, index_like_resp = get_split_random(index_likes, frac=resp_frac, margin=1)\n",
    "    if len(index_dilikes) > 0:\n",
    "        index_dislike_prompt, index_dislike_resp = get_split_random(index_dilikes, frac=resp_frac, margin=1)\n",
    "    row['index_like_prompt'] = index_like_prompt\n",
    "    row['index_like_resp'] = index_like_resp\n",
    "    row['index_dislike_prompt'] = index_dislike_prompt\n",
    "    row['index_dislike_resp'] = index_dislike_resp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group = test_group.swifter.apply(split_prompt_resp, axis=1, resp_frac=resp_frac)\n",
    "train_group = train_group.swifter.apply(split_prompt_resp, axis=1, resp_frac=resp_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='movielens':\n",
    "    def fix_title(title):\n",
    "        if \", The (\" in title:\n",
    "            name_film, _, year = title.rpartition(\", The (\")\n",
    "            title = \"The \" + name_film + \" (\" + year\n",
    "            return title\n",
    "        if \", A (\" in title:\n",
    "            name_film, _, year = title.rpartition(\", A (\")\n",
    "            title = \"A \" + name_film + \" (\" + year\n",
    "        return title\n",
    "    df_movies = pd.read_csv(os.path.join(datapath, r\"movies.dat\"), sep=\"::\", names=[\"item_id\", \"name\", \"geners\"], encoding='ISO-8859-1')\n",
    "    df_movies['name'] = df_movies['name'].apply(lambda x: fix_title(x)[0])\n",
    "    #df_movies['year'] = df_movies['name'].apply(lambda x: fix_title(x)[1])\n",
    "    import re\n",
    "\n",
    "    df_relations = pd.read_csv(os.path.join(datapath, 'mapping_entities.tsv'), \\\n",
    "            names=['url', 'id_set'], sep='\\t')\n",
    "    dbpedia_mapping = pd.read_csv(os.path.join(datapath, 'MappingMovielens2DBpedia-1.2.tsv'), \\\n",
    "            names=['id_movie', 'name', 'dbpedia_url'], sep='\\t')\n",
    "    df_movies = dbpedia_mapping.set_index('dbpedia_url').join(df_relations.set_index('url')).reset_index()\n",
    "    df_movies['name'] = df_movies['name'].apply(lambda x: re.sub(\"\\s+\\(\\d+\\)$\", \"\", x))\n",
    "    df_movies['name'] = df_movies['name'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", ' ', x).strip())\n",
    "    df_movies.dropna(inplace=True)\n",
    "    df_movies['id_set'] = df_movies['id_set'].astype(int)\n",
    "    mapping_dict = df_movies.loc[:, ['id_set', 'name']].set_index('id_set').to_dict()['name']\n",
    "    property_mapping = pd.read_json(os.path.join(datapath, 'entities_names.json'), typ='series')\n",
    "    mapping_relation = pd.read_csv(os.path.join(datapath, 'mapping_relations.tsv'), \\\n",
    "            names=['rel', 'id'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='dbbook':\n",
    "    df_relations = pd.read_csv(os.path.join(datapath, 'mapping_entities.tsv'), sep='\\t')\n",
    "    df_relations['name'] = df_relations['uri'].apply(lambda x: x.split(\";\")[0])\n",
    "    temp = df_relations[df_relations['id'].isin(df['item'].unique())]\n",
    "    mapping_dict = temp.set_index('id').to_dict()['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='lastfm':\n",
    "    df_relations = pd.read_csv(os.path.join(datapath, 'mapping_items.tsv'), \\\n",
    "            names=['id', 'name'], sep='\\t')\n",
    "    mapping_dict = df_relations.set_index('id').to_dict()['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_desription = pd.read_csv(os.path.join(datapath, dataset+'.txt'), sep=';;', names=['item_id', 'description'],  on_bad_lines='skip')\n",
    "item_desription = item_desription[item_desription['item_id'].isin(all_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_desription['description'] = item_desription['description'].apply(lambda x: special_token + \" \" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_domain_kn.jsonl'), 'w') as outfile:\n",
    "    for new_id, desc in item_desription.to_dict()['description'].items():\n",
    "        json.dump({'target_id':new_id, 'text':desc}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_desc_dict = item_desription.to_dict()['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_desc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_with_names(row, col, mapping_dict):\n",
    "    return [mapping_dict.get(x, '') for x in np.array(row['item'])[row[col]] if len(mapping_dict.get(x, ''))>2]\n",
    "\n",
    "\n",
    "train_group['liked_prompt'] = train_group.apply(map_with_names, axis=1, col='index_like_prompt', mapping_dict=mapping_dict)\n",
    "train_group['liked_resp'] = train_group.apply(map_with_names, axis=1, col='index_like_resp', mapping_dict=mapping_dict)\n",
    "train_group['disliked_prompt'] = train_group.apply(map_with_names, axis=1, col='index_dislike_prompt', mapping_dict=mapping_dict)\n",
    "train_group['disliked_resp'] = train_group.apply(map_with_names, axis=1, col='index_dislike_resp', mapping_dict=mapping_dict)\n",
    "\n",
    "test_group['liked_prompt'] = test_group.apply(map_with_names, axis=1, col='index_like_prompt', mapping_dict=mapping_dict)\n",
    "test_group['liked_resp'] = test_group.apply(map_with_names, axis=1, col='index_like_resp', mapping_dict=mapping_dict)\n",
    "test_group['disliked_prompt'] = test_group.apply(map_with_names, axis=1, col='index_dislike_prompt', mapping_dict=mapping_dict)\n",
    "test_group['disliked_resp'] = test_group.apply(map_with_names, axis=1, col='index_dislike_resp', mapping_dict=mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for _, row in test_group.iterrows():\n",
    "    for item in row['liked_prompt']:\n",
    "        train.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 1\n",
    "        })\n",
    "    for item in row['disliked_prompt']:\n",
    "        train.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 0\n",
    "        })\n",
    "    for item in row['liked_resp']:\n",
    "        test.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 1\n",
    "        })\n",
    "    for item in row['disliked_resp']:\n",
    "        test.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 0\n",
    "        })\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "def prepare_user_prompt(row):\n",
    "    random.seed(22)\n",
    "    user_prompt = ''\n",
    "    liked_list = ', '\n",
    "    disliked_list = ', '\n",
    "    liked_part = \"\"\"I like the following items:\\n{liked_list}\\n\"\"\"\n",
    "    disliked_part = \"\"\"I dislike the following items:\\n{disliked_list}\\n\"\"\"\n",
    "    liked_items = sorted(row['liked_prompt'], key=lambda k: random.random())\n",
    "    disliked_items = sorted(row['disliked_prompt'], key=lambda k: random.random())\n",
    "    liked_list = liked_list.join(liked_items)\n",
    "    disliked_list = disliked_list.join(disliked_items)\n",
    "    if len(liked_items)>0:\n",
    "        user_prompt = user_prompt + liked_part.format(liked_list=liked_list)\n",
    "    if len(disliked_list)>0:\n",
    "        user_prompt = user_prompt + disliked_part.format(disliked_list=disliked_list)\n",
    "    \n",
    "    to_rank_list_str = ', '\n",
    "    response_part = \"\"\"\\nRank the following items:\\n{to_rank_list}\"\"\"\n",
    "    to_rank_list = list(itertools.chain(*[row['liked_resp'], row['disliked_resp']]))\n",
    "    to_rank_list = sorted(to_rank_list, key=lambda k: random.random())\n",
    "    to_rank_list = to_rank_list_str.join(to_rank_list)\n",
    "    user_prompt = user_prompt + response_part.format(to_rank_list=to_rank_list)\n",
    "    return user_prompt\n",
    "        \n",
    "def get_assistant_prompt(row):\n",
    "    ranked_list_str = ''\n",
    "    rank_part = \"\"\"Here a list:\\n{ranked_list}\\n\"\"\"\n",
    "    for i, x in enumerate(row['liked_resp']):\n",
    "        ranked_list_str = ranked_list_str + f\"{i+1}. {x}\\n\"\n",
    "    for i, x in enumerate(row['disliked_resp'], len(row['liked_resp'])):\n",
    "        ranked_list_str = ranked_list_str + f\"{i+1}. {x}\\n\"\n",
    "    return rank_part.format(ranked_list=ranked_list_str)[:-1]\n",
    "\n",
    "\n",
    "train_group['user_prompt'] = train_group.apply(prepare_user_prompt, axis=1)\n",
    "train_group['assistant_prompt'] = train_group.apply(get_assistant_prompt, axis=1)\n",
    "\n",
    "test_group['user_prompt'] = test_group.apply(prepare_user_prompt, axis=1)\n",
    "test_group['assistant_prompt'] = test_group.apply(get_assistant_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_group.loc[150, 'user_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_group.loc[150, 'assistant_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.chat_template =  \"{% set ns = namespace(i=0) %}\" \\\n",
    "#                            \"{% for message in messages %}\" \\\n",
    "#                                \"{% if message['role'] == 'system' and ns.i == 0 %}\" \\\n",
    "#                                       \"{{ bos_token +' [INST] <<SYS>>\\n' }}\" \\\n",
    "#                                       \"{{ message['content'] + ' <</SYS>>\\n'}}\" \\\n",
    "#                                \"{% elif message['role'] == 'user' %}\" \\\n",
    "#                                    \"{{ message['content'] + ' [/INST]\\n'}}\" \\\n",
    "#                                \"<|reserved_special_token_22|>\\n\"\\\n",
    "#                                \"{% elif message['role'] == 'assistant' %}\" \\\n",
    "#                                    \"{{ message['content'] + ' ' + eos_token }}\" \\\n",
    "#                                \"{% endif %}\" \\\n",
    "#                                \"{% set ns.i = ns.i+1 %}\" \\\n",
    "#                            \"{% endfor %}\"\n",
    "#\n",
    "tokenizer.chat_template =  \"{% set ns = namespace(i=0) %}\" \\\n",
    "                            \"{% for message in messages %}\" \\\n",
    "                                \"{% if message['role'] == 'system' and ns.i == 0 %}\" \\\n",
    "                                       \"{{ bos_token +' [INST] <<SYS>>\\n' }}\" \\\n",
    "                                       \"{{ message['content'] + ' <</SYS>>\\n'}}\" \\\n",
    "                                \"{% elif message['role'] == 'user' %}\" \\\n",
    "                                    \"{{ message['content'] + ' [/INST]\\n'}}\" \\\n",
    "                                \"{% elif message['role'] == 'assistant' %}\" \\\n",
    "                                    \"{{ message['content'] + '' + eos_token }}\" \\\n",
    "                                \"{% endif %}\" \\\n",
    "                                \"{% set ns.i = ns.i+1 %}\" \\\n",
    "                            \"{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(row, is_train=True):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"You're operating as a {domain} recommendation system. Your task involves receiving a user's list of liked and disliked items, along with a set of candidate items, and then reordering them based on preferences.\"},\n",
    "    {\"role\": \"user\", \"content\": row['user_prompt']}\n",
    "    ]\n",
    "    if is_train:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": row['assistant_prompt']})\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_gpt(row, is_train=True):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"You're operating as a {domain} recommendation system. Your task involves receiving a user's list of liked and disliked items, along with a set of candidate items, and then reordering them based on preferences.\"},\n",
    "    {\"role\": \"user\", \"content\": row['user_prompt']}\n",
    "    ]\n",
    "    if is_train:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": row['assistant_prompt']})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(test_group.apply(get_prompt_gpt, axis=1, is_train=False).tolist(), os.path.join(datapath, \"gpt_prompts.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group['prompt'] = train_group.apply(get_prompt, axis=1, is_train=True)\n",
    "test_group['prompt'] = test_group.apply(get_prompt, axis=1, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_liked(row):\n",
    "    liked_item_index = row['index_like_resp']\n",
    "    likex_items = np.array(row['item'])[liked_item_index]\n",
    "    return '\\n'.join([item_desc_dict.get(y, '') for y in likex_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group['descriptions'] = train_group['item'].apply(lambda x: [item_desc_dict.get(y, '') for y in x if len(item_desc_dict.get(y, ''))>2])\n",
    "#train_group['descriptions'] = train_group.apply(get_description_liked, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.to_pickle(os.path.join(datapath, f'{dataset}_train_dataset.pkl'))\n",
    "test_group.to_pickle(os.path.join(datapath, f'{dataset}_test_dataset.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = train_group['prompt'].tolist() #* 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(item_desc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = [f'{tokenizer.bos_token} ' + x + f' {tokenizer.eos_token}' for x in descriptions if type(x)==str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_text_kn_train_set.jsonl'), 'w') as outfile:\n",
    "    for x in descriptions:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.seed(22)\n",
    "train_set = prompts*3 + descriptions\n",
    "\n",
    "random.shuffle(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set_no_kn.jsonl'), 'w') as outfile:\n",
    "    for x in train_group['prompt'].tolist():\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set.jsonl'), 'w') as outfile:\n",
    "    for x in train_set:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = test_group['prompt'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_test_set.jsonl'), 'w') as outfile:\n",
    "    for x in prompts:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs = pd.concat([test_group.loc[:, ['user', 'index_like_prompt', 'index_dislike_prompt', 'item']],\\\n",
    "                      train_group.loc[:, ['user', 'index_like_prompt', 'index_dislike_prompt', 'item']]])\n",
    "test_bs = test_group.loc[:, ['user', 'index_like_resp', 'index_dislike_resp', 'item']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs['bs_format'] = train_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 1} for item in x['index_like_prompt']], axis=1)\n",
    "train_bs['bs_format'] = train_bs['bs_format'] + train_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 0} for item in x['index_dislike_prompt']], axis=1)\n",
    "\n",
    "test_bs['bs_format'] = test_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 1} for item in x['index_like_resp']], axis=1)\n",
    "test_bs['bs_format'] = test_bs['bs_format'] + test_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 0} for item in x['index_dislike_resp']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_set_bs = set(x['item_id:token'] for x in list(itertools.chain(*test_bs['bs_format'].tolist())) + list(itertools.chain(*train_bs['bs_format'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_bs = item_desription[item_desription['item_id'].isin(item_set_bs)].set_index('item_id').join(pd.DataFrame([{'id':k, 'name':v} for k, v in mapping_dict.items() if k in item_set_bs]).set_index('id')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_bs = ent_bs.rename(columns={\n",
    "    'item_id':'item_id:token',\n",
    "    'name': 'title:token_seq',\n",
    "    'description': 'description:token_seq',\n",
    "})\n",
    "\n",
    "ent_bs.to_csv(os.path.join(datapath, 'baseline', f'{dataset}.item'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "os.makedirs(os.path.join(datapath, 'baseline'), exist_ok=True)\n",
    "pd.DataFrame(list(itertools.chain(*test_bs['bs_format'].tolist()))).to_csv(os.path.join(datapath, 'baseline', f'{dataset}.part3.inter'), sep='\\t', index=False)\n",
    "pd.DataFrame(list(itertools.chain(*train_bs['bs_format'].tolist()))).to_csv(os.path.join(datapath, 'baseline', f'{dataset}.part1.inter'), sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
