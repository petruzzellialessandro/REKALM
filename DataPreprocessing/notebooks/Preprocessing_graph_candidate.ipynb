{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dbbook'\n",
    "\n",
    "domain = {\n",
    "    'lastfm': 'music',\n",
    "    'dbbook': 'book',\n",
    "    'movielens': 'movie'\n",
    "}[dataset]\n",
    "\n",
    "items = {\n",
    "    'lastfm': 'artist',\n",
    "    'dbbook': 'book',\n",
    "    'movielens': 'movie'\n",
    "}[dataset]\n",
    "import os\n",
    "datapath = os.path.join('..', 'data', dataset)\n",
    "\n",
    "special_token = '<|reserved_special_token_22|>'\n",
    "\n",
    "resp_frac = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import swifter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_train = pd.read_csv(os.path.join(datapath, 'user-item', 'train.tsv'), \\\n",
    "            names=['user', 'item', 'rating'], sep='\\t')\n",
    "\n",
    "user_item_df_test = pd.read_csv(os.path.join(datapath, 'user-item', 'test.tsv'), \\\n",
    "            names=['user', 'item', 'rating'], sep='\\t')\n",
    "\n",
    "\n",
    "df = pd.concat([user_item_df_train, user_item_df_test], ignore_index=True)\n",
    "all_items = df['item'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_random(list_to_split, frac=0.2, margin=0):\n",
    "    random.seed(22)\n",
    "    sample_list = random.sample(list_to_split, int(frac*len(list_to_split))+margin)\n",
    "    complementary_set = set(list_to_split) - set(sample_list)\n",
    "    return list(set(complementary_set)), list(set(sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(22)\n",
    "test_frac = 0.2\n",
    "user_list = df['user'].unique().tolist()\n",
    "user_train, user_test = get_split_random(user_list, frac=test_frac, margin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_train = df[df['user'].isin(user_train)]\n",
    "user_item_df_test = df[df['user'].isin(user_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(user_item_df_train['user'].unique()) - set(user_item_df_test['user'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_test['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df_train['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group = user_item_df_train.groupby('user').agg({\n",
    "    'item': list,\n",
    "    'rating': list\n",
    "}).reset_index()\n",
    "test_group = user_item_df_test.groupby('user').agg({\n",
    "    'item': list,\n",
    "    'rating': list\n",
    "}).reset_index()\n",
    "train_group['len'] = train_group['item'].apply(len)\n",
    "test_group['len'] = test_group['item'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_prompt_resp(row, resp_frac):\n",
    "    items = np.array(row['rating'])\n",
    "    index_likes = np.where(items==1)[0].tolist()\n",
    "    index_dilikes = np.where(items==0)[0].tolist()\n",
    "    index_like_prompt = []\n",
    "    index_like_resp = []    \n",
    "    index_dislike_prompt = []\n",
    "    index_dislike_resp = []\n",
    "    resp_frac=resp_frac\n",
    "    if len(index_likes) > 0:\n",
    "        index_like_prompt, index_like_resp = get_split_random(index_likes, frac=resp_frac, margin=1)\n",
    "    if len(index_dilikes) > 0:\n",
    "        index_dislike_prompt, index_dislike_resp = get_split_random(index_dilikes, frac=resp_frac, margin=1)\n",
    "    row['index_like_prompt'] = index_like_prompt\n",
    "    row['index_like_resp'] = index_like_resp\n",
    "    row['index_dislike_prompt'] = index_dislike_prompt\n",
    "    row['index_dislike_resp'] = index_dislike_resp\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group = test_group.swifter.apply(split_prompt_resp, axis=1, resp_frac=resp_frac)\n",
    "train_group = train_group.swifter.apply(split_prompt_resp, axis=1, resp_frac=resp_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='movielens':\n",
    "    def fix_title(title):\n",
    "        if \", The (\" in title:\n",
    "            name_film, _, year = title.rpartition(\", The (\")\n",
    "            title = \"The \" + name_film + \" (\" + year\n",
    "            return title\n",
    "        if \", A (\" in title:\n",
    "            name_film, _, year = title.rpartition(\", A (\")\n",
    "            title = \"A \" + name_film + \" (\" + year\n",
    "        return title\n",
    "    df_movies = pd.read_csv(os.path.join(datapath, r\"movies.dat\"), sep=\"::\", names=[\"item_id\", \"name\", \"geners\"], encoding='ISO-8859-1')\n",
    "    df_movies['name'] = df_movies['name'].apply(lambda x: fix_title(x)[0])\n",
    "    #df_movies['year'] = df_movies['name'].apply(lambda x: fix_title(x)[1])\n",
    "    import re\n",
    "\n",
    "    df_relations = pd.read_csv(os.path.join(datapath, 'mapping_entities.tsv'), \\\n",
    "            names=['url', 'id_set'], sep='\\t')\n",
    "    dbpedia_mapping = pd.read_csv(os.path.join(datapath, 'MappingMovielens2DBpedia-1.2.tsv'), \\\n",
    "            names=['id_movie', 'name', 'dbpedia_url'], sep='\\t')\n",
    "    df_movies = dbpedia_mapping.set_index('dbpedia_url').join(df_relations.set_index('url')).reset_index()\n",
    "    df_movies['name'] = df_movies['name'].apply(lambda x: re.sub(\"\\s+\\(\\d+\\)$\", \"\", x))\n",
    "    df_movies['name'] = df_movies['name'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", ' ', x).strip())\n",
    "    df_movies.dropna(inplace=True)\n",
    "    df_movies['id_set'] = df_movies['id_set'].astype(int)\n",
    "    mapping_dict = df_movies.loc[:, ['id_set', 'name']].set_index('id_set').to_dict()['name']\n",
    "    property_mapping = pd.read_json(os.path.join(datapath, 'entities_names.json'), typ='series')\n",
    "    mapping_relation = pd.read_csv(os.path.join(datapath, 'mapping_relations.tsv'), \\\n",
    "            names=['rel', 'id'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='dbbook':\n",
    "    df_relations = pd.read_csv(os.path.join(datapath, 'mapping_entities.tsv'), sep='\\t')\n",
    "    df_relations['name'] = df_relations['uri'].apply(lambda x: x.split(\";\")[0])\n",
    "    temp = df_relations[df_relations['id'].isin(df['item'].unique())]\n",
    "    mapping_dict = temp.set_index('id').to_dict()['name']\n",
    "    property_mapping = pd.read_json(os.path.join(datapath, 'entities_names.json'), typ='series')\n",
    "    mapping_relation = pd.read_csv(os.path.join(datapath, 'mapping_relations.tsv'), \\\n",
    "            names=['id', 'rel'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='lastfm':\n",
    "    df_relations = pd.read_csv(os.path.join(datapath, 'mapping_items.tsv'), \\\n",
    "            names=['id', 'name'], sep='\\t')\n",
    "    mapping_dict = df_relations.set_index('id').to_dict()['name']\n",
    "    property_mapping = pd.read_json(os.path.join(datapath, 'entities_names.json'), typ='series')\n",
    "    mapping_relation = pd.read_csv(os.path.join(datapath, 'mapping_relations.tsv'), \\\n",
    "            names=['rel', 'id'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='boardgamegeek':\n",
    "    df_relations = pd.read_csv(os.path.join(datapath, 'mapped_entities.csv'), sep=';')\n",
    "    df_relations['name'] = df_relations['primary']\n",
    "    mapping_dict = df_relations.set_index('id').to_dict()['name']\n",
    "    property_mapping = pd.read_csv(os.path.join(datapath, 'mapped_prop.csv'), sep=';;', names=['id', 'name'])\n",
    "    property_mapping = property_mapping.set_index('id').to_dict()['name'] \n",
    "    property_mapping = pd.Series(property_mapping) \n",
    "    mapping_relation = pd.read_csv(os.path.join(datapath, 'mapped_rel.csv'), \\\n",
    "            names=['id', 'rel'], sep=';;', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def parse_name_from_url(url):\n",
    "    name = url.split(\"/\")[-1]\n",
    "    name = name.replace(\"_\", \" \")\n",
    "    name = name.split(':')[-1]\n",
    "    name = re.sub(r\"\\([^()]*\\)\", \"\", name)\n",
    "    pattern = r\",.*$\"\n",
    "    name = re.sub(pattern, \"\", name)\n",
    "    name = name[0].upper() + name[1:]\n",
    "    name = \" \".join(re.findall('[A-Z][a-z]*', name))\n",
    "    return name.strip().capitalize()\n",
    "\n",
    "mapping_relation['name_rel'] = mapping_relation['rel'].apply(parse_name_from_url)\n",
    "like_dislike_ids = mapping_relation[mapping_relation['name_rel'].isin(['Like', 'Dislike'])]['id'].tolist()\n",
    "files_prop = [x for x in os.listdir(os.path.join(datapath, 'item-prop')) if 'tsv' in x]\n",
    "property_graph = pd.concat([pd.read_csv(os.path.join(datapath, 'item-prop', f), \\\n",
    "            names=['item','prop','rel'], sep='\\t') for f in files_prop])\n",
    "property_graph = property_graph[~property_graph['rel'].isin(like_dislike_ids)]\n",
    "if dataset == 'lastfm':\n",
    "    property_graph.rename(columns={\n",
    "        'prop': 'rel', 'rel': 'prop'\n",
    "    }, inplace=True)\n",
    "property_graph = property_graph[property_graph['prop'].isin(property_mapping.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lessicalized_prop = {}\n",
    "if dataset=='dbbook':\n",
    "    lessicalized_prop['Author'] = 'The book was written by [Author].'\n",
    "    lessicalized_prop['Series'] =  'This book is part of a series called [Series].'\n",
    "    lessicalized_prop['Genre'] =  'This is a [Genre] novel.'\n",
    "    lessicalized_prop['Publisher'] = 'This book was published by [Publisher].'\n",
    "    lessicalized_prop['Preceded by'] = 'This book is preceded by [Preceded by].'\n",
    "    lessicalized_prop['Subject'] = 'The story is centered around the theme of [Subject].'\n",
    "    lessicalized_prop['Illustrator'] = 'The illustrations were done by [Illustrator].'\n",
    "    lessicalized_prop['Writers'] = 'The screenplay was written by [Writers].'\n",
    "    lessicalized_prop['Artists'] = 'The artwork was created by [Artists].'\n",
    "    lessicalized_prop['Letterers'] = 'The lettering was done by [Letterers].'\n",
    "    lessicalized_prop['Director'] = 'The film adaptation was directed by [Director].'\n",
    "    lessicalized_prop['Screenplay'] = 'The screenplay was written by [Screenplay].'\n",
    "    lessicalized_prop['Starring'] = 'The film stars [Starring].'\n",
    "    lessicalized_prop['Cinematography'] = 'The cinematography was handled by [Cinematography].'\n",
    "    lessicalized_prop['Producer'] = 'The film was produced by [Producer].'\n",
    "    lessicalized_prop['Distributor'] = 'The film was distributed by [Distributor].'\n",
    "    lessicalized_prop['Editor'] = 'This book was edited by [Editor].'\n",
    "    lessicalized_prop['Category'] = 'This book falls under the category of [Category].'\n",
    "    lessicalized_prop['Company'] = 'This book was published by [Company].'\n",
    "    lessicalized_prop['Writer'] = 'This book was written by [Writer].'\n",
    "    lessicalized_prop['Place'] = 'The story takes place in [Place].'\n",
    "    lessicalized_prop['Characters'] = 'The main characters include [Characters].'\n",
    "    lessicalized_prop['Setting'] = 'The story is set in [Setting].'\n",
    "    lessicalized_prop['Film adaption'] = 'This book has been adapted into a film titled [Film adaption].'\n",
    "    lessicalized_prop['Political'] = 'This book explores themes of [Political] politics.'\n",
    "    lessicalized_prop['Title'] = 'The title of the book is [Title].'\n",
    "    lessicalized_prop['Editors'] = 'This book was edited by [Editors].'\n",
    "    lessicalized_prop['Creators'] = 'This book was created by [Creators].'\n",
    "    lessicalized_prop['Magazine'] = 'This story was originally serialized in [Magazine].'\n",
    "    lessicalized_prop['Studio'] = 'The film adaptation was produced by [Studio].'\n",
    "    lessicalized_prop['Network'] = 'This book has been adapted into a TV series on [Network].'\n",
    "    lessicalized_prop['Narrated'] = 'The audiobook is narrated by [Narrated].'\n",
    "    lessicalized_prop['Creator'] = 'This work was created by [Creator].'\n",
    "    lessicalized_prop['Published'] = 'The book was published in [Published].'\n",
    "    lessicalized_prop['Authors'] = 'This book was written by the following authors: [Authors].'\n",
    "    lessicalized_prop['Co author'] = 'The author co-authored this book with [Co-author name].'\n",
    "\n",
    "if dataset == 'lastfm':\n",
    "    lessicalized_prop['Subject'] = 'The artist\\'s work focuses primarily on themes of [Subject].'\n",
    "    lessicalized_prop['Genre'] = 'The artist is known for their unique contribution to the [Genre] genre.'\n",
    "    lessicalized_prop['Current members'] = 'Currently, the artist\\'s lineup includes [Current members].'\n",
    "    lessicalized_prop['Origin'] = 'The artist hails from [Origin], bringing a distinctive sound to their music.'\n",
    "    lessicalized_prop['Past members'] = 'Over the years, the artist has seen several changes, with past members including [Past members].'\n",
    "    lessicalized_prop['Occupation'] = 'In addition to being a musician, the artist also works as [Occupation].'\n",
    "    lessicalized_prop['Instrument'] = 'The artist is proficient in playing the [Instrument].'\n",
    "    lessicalized_prop['Genres'] = 'Their music spans across multiple genres, including [Genres].'\n",
    "    lessicalized_prop['Home town'] = 'The artist grew up in [Home town], which has greatly influenced their music.'\n",
    "    lessicalized_prop['Religion'] = 'The artist\\'s work is often inspired by their religious beliefs in [Religion].'\n",
    "    lessicalized_prop['Partner'] = 'In their personal life, the artist is partnered with [Partner].'\n",
    "    lessicalized_prop['Alias'] = 'The artist is also known by the alias [Alias].'\n",
    "    lessicalized_prop['Title'] = 'The artist holds the title of [Title] in the music industry.'\n",
    "    lessicalized_prop['Author'] = 'They are also an accomplished author, having written [Author].'\n",
    "    lessicalized_prop['Work'] = 'The artist\\'s notable works include [Work].'\n",
    "    lessicalized_prop['Notable instruments'] = 'They are renowned for their skill with notable instruments such as [Notable instruments].'\n",
    "    lessicalized_prop['Manager'] = 'The artist is managed by [Manager].'\n",
    "    lessicalized_prop['Former members'] = 'Former members of the artist\\'s ensemble include [Former members].'\n",
    "    lessicalized_prop['Voice type'] = 'The artist is recognized for their [Voice type] voice.'\n",
    "    lessicalized_prop['Siblings'] = 'The artist has [Siblings] siblings who have also influenced their music career.'\n",
    "    lessicalized_prop['Nationality'] = 'They proudly represent their [Nationality] heritage in their music.'\n",
    "    lessicalized_prop['Instruments'] = 'The artist is adept at playing several instruments, including [Instruments].'\n",
    "    lessicalized_prop['Television'] = 'In addition to music, the artist has appeared on television shows such as [Television].'\n",
    "    lessicalized_prop['Influences'] = 'Their musical influences include [Influences].'\n",
    "    lessicalized_prop['Agency'] = 'The artist is represented by the agency [Agency].'\n",
    "    lessicalized_prop['Record labels'] = 'They have been signed with record labels such as [Record labels].'\n",
    "    lessicalized_prop['Touring members'] = 'When touring, the artist is accompanied by members such as [Touring members].'\n",
    "    lessicalized_prop['Former labels'] = 'The artist has previously been associated with former labels like [Former labels].'\n",
    "    lessicalized_prop['Trainer'] = 'Their skills have been honed under the guidance of their trainer, [Trainer].'\n",
    "\n",
    "if dataset=='movielens':\n",
    "    lessicalized_prop['Starring'] = 'In this movie, [Starring]  take on a prominent role.'\n",
    "    lessicalized_prop['Producer'] = 'Producing this movie is [Producer].'\n",
    "    lessicalized_prop['Language'] = 'This movie is primarily in [Language].'\n",
    "    lessicalized_prop['Editing'] = 'The editing in this movie is handled by [Editing].'\n",
    "    lessicalized_prop['Country'] = 'This movie is set in [Country].'\n",
    "    lessicalized_prop['Music composer'] = 'The musics of this movie is composed by [Music composer].'\n",
    "    lessicalized_prop['Based on'] = 'This movie is based on [Based on].'\n",
    "    lessicalized_prop['Subject'] = 'this movie explores [Subject] themes.'\n",
    "    lessicalized_prop['Cinematography'] = 'The visuals of this movie are captured by [Cinematography].'\n",
    "    lessicalized_prop['Writer'] = 'The narrative of this movie is penned by [Writer].'\n",
    "    lessicalized_prop['Director'] = 'Guiding the vision of this movie is the director, [Director].'\n",
    "\n",
    "if dataset=='boardgamegeek':\n",
    "    lessicalized_prop['Type'] = 'This game falls under the [Type] category.'\n",
    "    lessicalized_prop['Primary'] = 'The primary feature of this game is [Primary].'\n",
    "    lessicalized_prop['Alternate'] = 'An alternate name for this game is [Alternate].'\n",
    "    lessicalized_prop['Yearpublished'] = 'This game was published in the year [Yearpublished].'\n",
    "    lessicalized_prop['Minplayers'] = 'The minimum number of players required is [Minplayers].'\n",
    "    lessicalized_prop['Maxplayers'] = 'The maximum number of players allowed is [Maxplayers].'\n",
    "    lessicalized_prop['Playingtime'] = 'The average playing time for this game is [Playingtime] minutes.'\n",
    "    lessicalized_prop['Minplaytime'] = 'The minimum playtime for this game is [Minplaytime] minutes.'\n",
    "    lessicalized_prop['Maxplaytime'] = 'The maximum playtime for this game is [Maxplaytime] minutes.'\n",
    "    lessicalized_prop['Minage'] = 'The minimum age recommended to play this game is [Minage] years.'\n",
    "    lessicalized_prop['Boardgamecategory'] = 'This game belongs to the [Boardgamecategory] category.'\n",
    "    lessicalized_prop['Boardgamemechanic'] = 'The main mechanic of this game is [Boardgamemechanic].'\n",
    "    lessicalized_prop['Boardgamefamily'] = 'This game is part of the [Boardgamefamily] family.'\n",
    "    lessicalized_prop['Boardgameexpansion'] = '[Boardgameexpansion] is an expansion for this game.'\n",
    "    lessicalized_prop['Boardgameimplementation'] = 'This game implements [Boardgameimplementation].'\n",
    "    lessicalized_prop['Boardgamedesigner'] = 'The designer of this game is [Boardgamedesigner].'\n",
    "    lessicalized_prop['Boardgameartist'] = 'The artist for this game is [Boardgameartist].'\n",
    "    lessicalized_prop['Boardgamepublisher'] = 'This game is published by [Boardgamepublisher].'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_name_rel =property_graph.set_index('rel').join(mapping_relation.loc[:, ['name_rel', 'id']].set_index('id')).reset_index()\n",
    "goupd_mapping = mapping_name_rel.groupby(['item', 'name_rel']).agg({\n",
    "    'prop':list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = goupd_mapping.reset_index()\n",
    "kn_lex = {}\n",
    "property_mapping_dict = property_mapping.to_dict()\n",
    "for item in temp['item'].unique():\n",
    "    item_df = temp[temp['item'] == item].sort_values('name_rel')\n",
    "    item_name = mapping_dict.get(item, None)\n",
    "    if item_name == None:\n",
    "        continue\n",
    "    prompt = f'{item_name} is a {items}.'\n",
    "    for _, row in item_df.iterrows():\n",
    "        names_property = [property_mapping_dict[x] for x in row['prop'] if property_mapping_dict.get(x, None)!=None]\n",
    "        prompt += f\" {lessicalized_prop[row['name_rel']]}\"\n",
    "        if len(names_property) > 1:\n",
    "            names_property_str = \", \".join(names_property)\n",
    "        else:\n",
    "            names_property_str = names_property[0]\n",
    "        prompt = prompt.replace(f\"[{row['name_rel']}]\", names_property_str)\n",
    "    kn_lex[item] = prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_domain_kn_graph.jsonl'), 'w') as outfile:\n",
    "    for new_id, desc in kn_lex.items():\n",
    "        json.dump({'target_id':str(new_id), 'text':desc}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_desc_df = pd.read_json(os.path.join(datapath, f'{dataset}_domain_kn_graph.jsonl'), lines=True) \n",
    "item_desc_df['target_id'] = item_desc_df['target_id'].astype(int)\n",
    "item_desc_dict = item_desc_df.set_index('target_id').to_dict()['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict[8670]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_with_names(row, col, mapping_dict):\n",
    "    return [mapping_dict.get(x, '') for x in np.array(row['item'])[row[col]] if len(mapping_dict.get(x, ''))>2]\n",
    "\n",
    "\n",
    "train_group['liked_prompt'] = train_group.apply(map_with_names, axis=1, col='index_like_prompt', mapping_dict=mapping_dict)\n",
    "train_group['liked_resp'] = train_group.apply(map_with_names, axis=1, col='index_like_resp', mapping_dict=mapping_dict)\n",
    "train_group['disliked_prompt'] = train_group.apply(map_with_names, axis=1, col='index_dislike_prompt', mapping_dict=mapping_dict)\n",
    "train_group['disliked_resp'] = train_group.apply(map_with_names, axis=1, col='index_dislike_resp', mapping_dict=mapping_dict)\n",
    "\n",
    "test_group['liked_prompt'] = test_group.apply(map_with_names, axis=1, col='index_like_prompt', mapping_dict=mapping_dict)\n",
    "test_group['liked_resp'] = test_group.apply(map_with_names, axis=1, col='index_like_resp', mapping_dict=mapping_dict)\n",
    "test_group['disliked_prompt'] = test_group.apply(map_with_names, axis=1, col='index_dislike_prompt', mapping_dict=mapping_dict)\n",
    "test_group['disliked_resp'] = test_group.apply(map_with_names, axis=1, col='index_dislike_resp', mapping_dict=mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for _, row in test_group.iterrows():\n",
    "    for item in row['liked_prompt']:\n",
    "        train.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 1\n",
    "        })\n",
    "    for item in row['disliked_prompt']:\n",
    "        train.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 0\n",
    "        })\n",
    "    for item in row['liked_resp']:\n",
    "        test.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 1\n",
    "        })\n",
    "    for item in row['disliked_resp']:\n",
    "        test.append({\n",
    "            'user_id': row['user'],\n",
    "            'item_id': item,\n",
    "            'score': 0\n",
    "        })\n",
    "\n",
    "pd.DataFrame(train).to_csv(\"train_adaptation.csv\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "def prepare_user_prompt(row):\n",
    "    random.seed(22)\n",
    "    user_prompt = ''\n",
    "    liked_list = ', '\n",
    "    disliked_list = ', '\n",
    "    liked_part = \"\"\"I like the following items:\\n{liked_list}\\n\"\"\"\n",
    "    disliked_part = \"\"\"I dislike the following items:\\n{disliked_list}\\n\"\"\"\n",
    "    liked_items = sorted(row['liked_prompt'], key=lambda k: random.random())\n",
    "    disliked_items = sorted(row['disliked_prompt'], key=lambda k: random.random())\n",
    "    liked_list = liked_list.join(liked_items)\n",
    "    disliked_list = disliked_list.join(disliked_items)\n",
    "    if len(liked_items)>0:\n",
    "        user_prompt = user_prompt + liked_part.format(liked_list=liked_list)\n",
    "    if len(disliked_list)>0:\n",
    "        user_prompt = user_prompt + disliked_part.format(disliked_list=disliked_list)\n",
    "    \n",
    "    to_rank_list_str = ', '\n",
    "    response_part = \"\"\"\\nRank the following items:\\n{to_rank_list}\"\"\"\n",
    "    to_rank_list = list(itertools.chain(*[row['liked_resp'], row['disliked_resp']]))\n",
    "    to_rank_list = sorted(to_rank_list, key=lambda k: random.random())\n",
    "    to_rank_list = to_rank_list_str.join(to_rank_list)\n",
    "    user_prompt = user_prompt + response_part.format(to_rank_list=to_rank_list)\n",
    "    return user_prompt\n",
    "        \n",
    "def get_assistant_prompt(row):\n",
    "    ranked_list_str = ''\n",
    "    rank_part = \"\"\"Here a list:\\n{ranked_list}\\n\"\"\"\n",
    "    for i, x in enumerate(row['liked_resp']):\n",
    "        ranked_list_str = ranked_list_str + f\"{i+1}. {x}\\n\"\n",
    "    for i, x in enumerate(row['disliked_resp'], len(row['liked_resp'])):\n",
    "        ranked_list_str = ranked_list_str + f\"{i+1}. {x}\\n\"\n",
    "    return rank_part.format(ranked_list=ranked_list_str)[:-1]\n",
    "\n",
    "\n",
    "train_group['user_prompt'] = train_group.apply(prepare_user_prompt, axis=1)\n",
    "train_group['assistant_prompt'] = train_group.apply(get_assistant_prompt, axis=1)\n",
    "\n",
    "test_group['user_prompt'] = test_group.apply(prepare_user_prompt, axis=1)\n",
    "test_group['assistant_prompt'] = test_group.apply(get_assistant_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_group.loc[150, 'user_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_group.loc[150, 'assistant_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.chat_template =  \"{% set ns = namespace(i=0) %}\" \\\n",
    "#                            \"{% for message in messages %}\" \\\n",
    "#                                \"{% if message['role'] == 'system' and ns.i == 0 %}\" \\\n",
    "#                                       \"{{ bos_token +' [INST] <<SYS>>\\n' }}\" \\\n",
    "#                                       \"{{ message['content'] + ' <</SYS>>\\n'}}\" \\\n",
    "#                                \"{% elif message['role'] == 'user' %}\" \\\n",
    "#                                    \"{{ message['content'] + ' [/INST]\\n'}}\" \\\n",
    "#                                \"<|reserved_special_token_22|>\\n\"\\\n",
    "#                                \"{% elif message['role'] == 'assistant' %}\" \\\n",
    "#                                    \"{{ message['content'] + ' ' + eos_token }}\" \\\n",
    "#                                \"{% endif %}\" \\\n",
    "#                                \"{% set ns.i = ns.i+1 %}\" \\\n",
    "#                            \"{% endfor %}\"\n",
    "#\n",
    "tokenizer.chat_template =  \"{% set ns = namespace(i=0) %}\" \\\n",
    "                            \"{% for message in messages %}\" \\\n",
    "                                \"{% if message['role'] == 'system' and ns.i == 0 %}\" \\\n",
    "                                       \"{{ bos_token +' [INST] <<SYS>>\\n' }}\" \\\n",
    "                                       \"{{ message['content'] + ' <</SYS>>\\n'}}\" \\\n",
    "                                \"{% elif message['role'] == 'user' %}\" \\\n",
    "                                    \"{{ message['content'] + ' [/INST]\\n'}}\" \\\n",
    "                                \"{% elif message['role'] == 'assistant' %}\" \\\n",
    "                                    \"{{ message['content'] + '' + eos_token }}\" \\\n",
    "                                \"{% endif %}\" \\\n",
    "                                \"{% set ns.i = ns.i+1 %}\" \\\n",
    "                            \"{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(row, is_train=True):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"You're operating as a {domain} recommendation system. Your task involves receiving a user's list of liked and disliked items, along with a set of candidate items, and then reordering them based on preferences.\"},\n",
    "    {\"role\": \"user\", \"content\": row['user_prompt']}\n",
    "    ]\n",
    "    if is_train:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": row['assistant_prompt']})\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group['prompt'] = train_group.apply(get_prompt, axis=1, is_train=True)\n",
    "test_group['prompt'] = test_group.apply(get_prompt, axis=1, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_liked(row):\n",
    "    liked_item_index = row['index_like_resp']\n",
    "    likex_items = np.array(row['item'])[liked_item_index]\n",
    "    return '\\n'.join([item_desc_dict.get(y, '') for y in likex_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group['descriptions'] = train_group['item'].apply(lambda x: [item_desc_dict.get(y, '') for y in x if len(item_desc_dict.get(y, ''))>2])\n",
    "#train_group['descriptions'] = train_group.apply(get_description_liked, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group.to_pickle(os.path.join(datapath, f'{dataset}_train_dataset_graph.pkl'))\n",
    "test_group.to_pickle(os.path.join(datapath, f'{dataset}_test_dataset_graph.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = train_group['prompt'].tolist() #* 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(item_desc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = [f'{tokenizer.bos_token} ' + x + f' {tokenizer.eos_token}' for x in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_graph_kn_train_set.jsonl'), 'w') as outfile:\n",
    "    for x in descriptions:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.seed(22)\n",
    "train_set = prompts + descriptions* 3\n",
    "\n",
    "random.shuffle(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set_kgraph.jsonl'), 'w') as outfile:\n",
    "    for x in train_set:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set_no_kn.jsonl'), 'w') as outfile:\n",
    "    for x in train_group['prompt'].tolist():\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = test_group['prompt'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_test_set.jsonl'), 'w') as outfile:\n",
    "    for x in prompts:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs = pd.concat([test_group.loc[:, ['user', 'index_like_prompt', 'index_dislike_prompt', 'item']],\\\n",
    "                      train_group.loc[:, ['user', 'index_like_prompt', 'index_dislike_prompt', 'item']]])\n",
    "test_bs = test_group.loc[:, ['user', 'index_like_resp', 'index_dislike_resp', 'item']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs['bs_format'] = train_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 1} for item in x['index_like_prompt']], axis=1)\n",
    "train_bs['bs_format'] = train_bs['bs_format'] + train_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 0} for item in x['index_dislike_prompt']], axis=1)\n",
    "\n",
    "test_bs['bs_format'] = test_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 1} for item in x['index_like_resp']], axis=1)\n",
    "test_bs['bs_format'] = test_bs['bs_format'] + test_bs.apply(lambda x:[{'user_id:token': x['user'], 'item_id:token': x['item'][item], 'rating:float': 0} for item in x['index_dislike_resp']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_set_bs = set(x['item_id:token'] for x in list(itertools.chain(*test_bs['bs_format'].tolist())) + list(itertools.chain(*train_bs['bs_format'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_graph_bs = property_graph[property_graph['item'].isin(item_set_bs)].rename(columns={\n",
    "    'item':\"head_id:token\",\n",
    "    'rel': \"relation_id:token\",\n",
    "    'prop': \"tail_id:token\"\n",
    "})\n",
    "link_bs = property_graph_bs.loc[:, ['head_id:token']].copy()\n",
    "link_bs['item_id:token'] = link_bs.loc[:, ['head_id:token']].apply(lambda x:x)\n",
    "link_bs = link_bs.rename(columns={\n",
    "    'head_id:token': \"entity_id:token\",\n",
    "})\n",
    "\n",
    "property_graph_bs.loc[:, ['head_id:token', 'relation_id:token', 'tail_id:token']].to_csv(os.path.join(datapath, 'baseline', f'{dataset}.kg'), sep='\\t', index=False)\n",
    "link_bs.to_csv(os.path.join(datapath, 'baseline', f'{dataset}.link'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "os.makedirs(os.path.join(datapath, 'baseline'), exist_ok=True)\n",
    "pd.DataFrame(list(itertools.chain(*test_bs['bs_format'].tolist()))).to_csv(os.path.join(datapath, 'baseline', f'{dataset}.part3.inter'), sep='\\t', index=False)\n",
    "pd.DataFrame(list(itertools.chain(*train_bs['bs_format'].tolist()))).to_csv(os.path.join(datapath, 'baseline', f'{dataset}.part1.inter'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = np.array_split(train_group['descriptions'].to_list(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for split in splits:\n",
    "    tok = tokenizer(split.tolist(), return_length=True)\n",
    "    lens.append(tok['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.Series(list(chain(*lens))).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"<|reserved_special_token_22|>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset in ['dbbook', 'movielens']:\n",
    "    item_prop_df = pd.read_csv(os.path.join(datapath, 'item-prop', 'train.tsv'), \\\n",
    "                names=['item', 'prop', 'rel'], sep='\\t')\n",
    "else:\n",
    "    item_prop_df = pd.read_csv(os.path.join(datapath, 'item-prop', 'train.tsv'), \\\n",
    "                names=['item', 'rel', 'prop'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'dbbook':\n",
    "    mapping_rel = pd.read_csv(os.path.join(datapath, 'mapping_relations.tsv'), \\\n",
    "            names=['id_rel', 'name_rel'], sep='\\t')\n",
    "else:\n",
    "    mapping_rel = pd.read_csv(os.path.join(datapath, 'mapping_relations.tsv'), \\\n",
    "                names=['name_rel', 'id_rel'], sep='\\t')\n",
    "mapping_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_rel['name_rel_red']=mapping_rel['name_rel'].apply(lambda x: x.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_rel[mapping_rel['id_rel'].isin(item_prop_df['rel'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_desription = pd.read_csv(os.path.join(datapath, dataset+'.txt'), sep=';;', names=['item_id', 'description'],  on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_desription.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_desription['description'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_desription.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_desription[item_desription['item_id'].isin(user_item_df['item'].unique())]['description'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfromers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
