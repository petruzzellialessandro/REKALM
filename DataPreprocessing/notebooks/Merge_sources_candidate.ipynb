{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dbbook'\n",
    "\n",
    "domain = {\n",
    "    'lastfm': 'music',\n",
    "    'dbbook': 'book',\n",
    "    'movielens': 'movie'\n",
    "}[dataset]\n",
    "\n",
    "items = {\n",
    "    'lastfm': 'artist',\n",
    "    'dbbook': 'book',\n",
    "    'movielens': 'movie'\n",
    "}[dataset]\n",
    "import os\n",
    "datapath = os.path.join('..', 'data', dataset)\n",
    "\n",
    "special_token = '<|reserved_special_token_22|>'\n",
    "\n",
    "resp_frac = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import swifter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pd.read_pickle(os.path.join(datapath, f'{dataset}_train_dataset.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_desc_df = pd.read_json(os.path.join(datapath, f'{dataset}_domain_kn_graph.jsonl'), lines=True) \n",
    "item_desc_df['target_id'] = item_desc_df['target_id'].astype(int)\n",
    "item_desc_dict_graph = item_desc_df.set_index('target_id').to_dict()['text']\n",
    "\n",
    "item_desc_df = pd.read_json(os.path.join(datapath, f'{dataset}_domain_kn.jsonl'), lines=True) \n",
    "item_desc_df['target_id'] = item_desc_df['target_id'].astype(int)\n",
    "item_desc_dict_text = item_desc_df.set_index('target_id').to_dict()['text']\n",
    "\n",
    "item_desc_df = pd.read_json(os.path.join(datapath, f'{dataset}_domain_kn_collaborative.jsonl'), lines=True)\n",
    "collaborative_rules = item_desc_df.iloc[:, 0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../llama3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.chat_template =  \"{% set ns = namespace(i=0) %}\" \\\n",
    "#                            \"{% for message in messages %}\" \\\n",
    "#                                \"{% if message['role'] == 'system' and ns.i == 0 %}\" \\\n",
    "#                                       \"{{ bos_token +' [INST] <<SYS>>\\n' }}\" \\\n",
    "#                                       \"{{ message['content'] + ' <</SYS>>\\n'}}\" \\\n",
    "#                                \"{% elif message['role'] == 'user' %}\" \\\n",
    "#                                    \"{{ message['content'] + ' [/INST]\\n'}}\" \\\n",
    "#                                \"<|reserved_special_token_22|>\\n\"\\\n",
    "#                                \"{% elif message['role'] == 'assistant' %}\" \\\n",
    "#                                    \"{{ message['content'] + ' ' + eos_token }}\" \\\n",
    "#                                \"{% endif %}\" \\\n",
    "#                                \"{% set ns.i = ns.i+1 %}\" \\\n",
    "#                            \"{% endfor %}\"\n",
    "#\n",
    "tokenizer.chat_template =  \"{% set ns = namespace(i=0) %}\" \\\n",
    "                            \"{% for message in messages %}\" \\\n",
    "                                \"{% if message['role'] == 'system' and ns.i == 0 %}\" \\\n",
    "                                       \"{{ bos_token +' [INST] <<SYS>>\\n' }}\" \\\n",
    "                                       \"{{ message['content'] + ' <</SYS>>\\n'}}\" \\\n",
    "                                \"{% elif message['role'] == 'user' %}\" \\\n",
    "                                    \"{{ message['content'] + ' [/INST]\\n'}}\" \\\n",
    "                                \"{% elif message['role'] == 'assistant' %}\" \\\n",
    "                                    \"{{ message['content'] + '' + eos_token }}\" \\\n",
    "                                \"{% endif %}\" \\\n",
    "                                \"{% set ns.i = ns.i+1 %}\" \\\n",
    "                            \"{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = train_text['prompt'].tolist() * 3\n",
    "descriptions = list(item_desc_dict_graph.values()) + list(item_desc_dict_text.values()) \n",
    "descriptions = [f'{tokenizer.bos_token} ' + x + f' {tokenizer.eos_token}' for x in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.seed(22)\n",
    "train_set = prompts + descriptions\n",
    "\n",
    "random.shuffle(train_set)\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set_graph_text.jsonl'), 'w') as outfile:\n",
    "    for x in train_set:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = train_text['prompt'].tolist() * 3\n",
    "descriptions = list(item_desc_dict_text.values()) + list(collaborative_rules.values()) \n",
    "descriptions = [f'{tokenizer.bos_token} ' + x + f' {tokenizer.eos_token}' for x in descriptions]\n",
    "import random \n",
    "\n",
    "random.seed(22)\n",
    "train_set = prompts + descriptions\n",
    "\n",
    "random.shuffle(train_set)\n",
    "len(train_set)\n",
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set_collaborative_text.jsonl'), 'w') as outfile:\n",
    "    for x in train_set:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = train_text['prompt'].tolist() * 3\n",
    "descriptions = list(item_desc_dict_graph.values()) + list(collaborative_rules.values()) \n",
    "descriptions = [f'{tokenizer.bos_token} ' + x + f' {tokenizer.eos_token}' for x in descriptions]\n",
    "import random \n",
    "\n",
    "random.seed(22)\n",
    "train_set = prompts + descriptions\n",
    "\n",
    "random.shuffle(train_set)\n",
    "len(train_set)\n",
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set_collaborative_graph.jsonl'), 'w') as outfile:\n",
    "    for x in train_set:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = train_text['prompt'].tolist() * 3\n",
    "descriptions = list(item_desc_dict_graph.values()) + list(item_desc_dict_text.values()) + list(collaborative_rules.values()) \n",
    "descriptions = [f'{tokenizer.bos_token} ' + x + f' {tokenizer.eos_token}' for x in descriptions]\n",
    "import random \n",
    "\n",
    "random.seed(22)\n",
    "train_set = prompts + descriptions\n",
    "\n",
    "random.shuffle(train_set)\n",
    "len(train_set)\n",
    "import json\n",
    "\n",
    "with open(os.path.join(datapath, f'{dataset}_train_set_collaborative_graph_text.jsonl'), 'w') as outfile:\n",
    "    for x in train_set:\n",
    "        json.dump({'text':x}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfromers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
